{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "xlmbert-autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuyguA/data_science_scripts/blob/master/xlmbert_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWCzKzWZSf4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxbSIkOeZ-Sw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hmNmxdFZb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bdf666-ba74-4b9d-a935-38afa732b977"
      },
      "source": [
        "!sudo pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 12.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 57.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9f7de15cc8440bea24d2b02610f29e0d5dbb9c8a34be7e655a8be7850d14a428\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmjLPBxBZSgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "209c8114-60d4-4699-d7f9-ca8cd7141edf"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5lnCe43KKYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf004502-9368-41a3-f474-2866b059c876"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITYnYHv0gEPf"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Bidirectional, Dropout, Input, Embedding, GlobalMaxPool1D, TimeDistributed, RepeatVector, Lambda\n",
        "from keras import optimizers\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiuhW1kEgOHq"
      },
      "source": [
        "sentences = []\n",
        "\n",
        "lemmas = []\n",
        "analysis = []\n",
        "\n",
        "with open(\"turk.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        l = line.strip()\n",
        "        word, res = l.split(\" \")\n",
        "        word = word.strip()\n",
        "        lemma, ana = res.strip().split(\",\", 1)\n",
        "        analysis.append(ana.split(\",\"))\n",
        "        sentences.append(word)\n",
        "        lemmas.append(lemma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcXX9ZmdgPoi"
      },
      "source": [
        "sentences=sentences[:200]\n",
        "lemmas = lemmas[:200]\n",
        "analysis = analysis[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFuSLNK7gOyX"
      },
      "source": [
        "tokenizer = Tokenizer(char_level=True, filters=None, lower=True)\n",
        "\n",
        "tokenizer.fit_on_texts(lemmas)\n",
        "\n",
        "lemma_seqs = tokenizer.texts_to_sequences(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3m3fOyfjUuZ"
      },
      "source": [
        "atokenizer = Tokenizer(lower=True)\n",
        "atokenizer.fit_on_texts(analysis)\n",
        "anl_seqs = atokenizer.texts_to_sequences(analysis) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hEsJtIBgTIz"
      },
      "source": [
        "OUT_LEN = 20\n",
        "\n",
        "lemma_char = pad_sequences(lemma_seqs, OUT_LEN, padding=\"post\")\n",
        "\n",
        "\n",
        "\n",
        "lemmas = np.array(lemma_char)\n",
        "\n",
        "lemmas = lemmas.reshape(len(lemmas) , OUT_LEN, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vbuzCrsjniF"
      },
      "source": [
        "anl_word = pad_sequences(anl_seqs, 10, padding=\"post\")\n",
        "analysis = np.array(anl_word)\n",
        "analysis = analysis.reshape(200 , 10, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQXRh6fDgPDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0612badf-ecb2-42a1-f920-c0106cfe5f15"
      },
      "source": [
        "w =[xtokenizer.encode(x, add_special_tokens=True, max_length=30, pad_to_max_length=True )for x in sentences]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LZneeHtncUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMutdd-ToNVR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l1AvATiPmnr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dARL26YSoZDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cce869-7b5c-44e9-9083-368f4d7cc320"
      },
      "source": [
        "xtokenizer.encode(\"Hello!How are you madam?\", add_special_tokens=True, max_length=30, padding=True, return_tensors=\"tf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
              "array([[    0, 50944,   267, 18719,    95,    79,  5111,   209,   112,\n",
              "            1]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Trkl1QHmbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5734722d-0ceb-46bf-f9d4-ed4f88d2999c"
      },
      "source": [
        "from transformers import XLMTokenizer, TFXLMForSequenceClassification, TFXLMModel\n",
        "\n",
        "xtokenizer = XLMTokenizer.from_pretrained('xlm-mlm-17-1280')\n",
        "#xmodel = TFXLMForSequenceClassification.from_pretrained('xlm-mlm-17-1280')\n",
        "xmodel = TFXLMModel.from_pretrained(\"xlm-mlm-17-1280\", use_lang_emb=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlm-mlm-17-1280 were not used when initializing TFXLMModel: ['pred_layer_._proj']\n",
            "- This IS expected if you are initializing TFXLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLMModel were not initialized from the model checkpoint at xlm-mlm-17-1280 and are newly initialized: ['lang_embeddings/embeddings:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQsdN3o9GdoC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6G4Fw3jXKY6"
      },
      "source": [
        "w= np.array(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wIkR3DyaoGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67718161-61b0-474f-c8a4-569198dff6bf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXtLta3xYdXf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-XWQ1qAJn5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8de1a7f-d7b0-447b-cabe-ff5768a789b8"
      },
      "source": [
        "input_layer = Input(shape = (30,), dtype='int32')\n",
        "bert = xmodel(input_layer)\n",
        "bert = bert[0]\n",
        "\n",
        "lstm = LSTM(units=100, return_sequences=False)(bert)  # variational biLSTM\n",
        "rv = RepeatVector(OUT_LEN)(lstm)\n",
        "lstmd = LSTM(units=100, return_sequences=True)(rv)\n",
        "outputs = TimeDistributed(Dense(30, activation=\"softmax\"))(lstmd)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(w, lemmas, batch_size=32, epochs=5, verbose=1 )#, callbacks=[tensorboard_callback]), validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 30)]              0         \n",
            "_________________________________________________________________\n",
            "tfxlm_model (TFXLMModel)     ((None, 30, 1280),)       571518720 \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               552400    \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 20, 200)           160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 30)            6030      \n",
            "=================================================================\n",
            "Total params: 572,237,950\n",
            "Trainable params: 572,237,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "7/7 [==============================] - 77s 11s/step - loss: 3.2822 - sparse_categorical_accuracy: 0.5608\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 74s 11s/step - loss: 1.7075 - sparse_categorical_accuracy: 0.7728\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 75s 11s/step - loss: 1.2145 - sparse_categorical_accuracy: 0.7728\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.9927 - sparse_categorical_accuracy: 0.7728\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 74s 11s/step - loss: 0.8906 - sparse_categorical_accuracy: 0.7728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DzgK0ofHq33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01e0009e-7146-43ff-c5a9-dbf8cd647b32"
      },
      "source": [
        "sentences[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kendimizdenmişler'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubC8MZ_6iv7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30bb2ad-5126-45d7-c4e4-2e42149de74e"
      },
      "source": [
        "input_layer = Input(shape = (30,), dtype='int32')\n",
        "bert = xmodel(input_layer)\n",
        "bert = bert[0]\n",
        "\n",
        "lstm = LSTM(units=100, return_sequences=False)(bert)  # variational biLSTM\n",
        "rv = RepeatVector(30)(lstm)\n",
        "lstmd = LSTM(units=100, return_sequences=True)(rv)\n",
        "\n",
        "lemma_output = Lambda(lambda x: x[:,:20,] )(lstmd)\n",
        "analysis_output = Lambda(lambda x: x[:,20:,:])(lstmd)\n",
        "\n",
        "lemma_output = TimeDistributed(Dense(30, activation=\"softmax\"))(lemma_output)\n",
        "analysis_output = TimeDistributed(Dense(66, activation=\"softmax\"))(analysis_output)\n",
        "\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=[lemma_output, analysis_output])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(w, [lemmas, analysis], batch_size=32, epochs=5, verbose=1 )#, callbacks=[tensorboard_callback]), validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tfxlm_model (TFXLMModel)        ((None, 30, 1280),)  571518720   input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  (None, 100)          552400      tfxlm_model[8][0]                \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_5 (RepeatVector)  (None, 30, 100)      0           lstm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 30, 100)      80400       repeat_vector_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 20, 100)      0           lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10, 100)      0           lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 20, 30)       3030        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_5 (TimeDistrib (None, 10, 66)       6666        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 572,161,216\n",
            "Trainable params: 572,161,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_model/transformer/lang_embeddings/embeddings:0'] when minimizing the loss.\n",
            "7/7 [==============================] - 87s 12s/step - loss: 7.0603 - time_distributed_4_loss: 2.9940 - time_distributed_5_loss: 4.0663 - time_distributed_4_sparse_categorical_accuracy: 0.7218 - time_distributed_5_sparse_categorical_accuracy: 0.2780\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 82s 12s/step - loss: 4.4803 - time_distributed_4_loss: 1.1316 - time_distributed_5_loss: 3.3488 - time_distributed_4_sparse_categorical_accuracy: 0.7728 - time_distributed_5_sparse_categorical_accuracy: 0.3355\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 81s 12s/step - loss: 4.1593 - time_distributed_4_loss: 1.0866 - time_distributed_5_loss: 3.0727 - time_distributed_4_sparse_categorical_accuracy: 0.7728 - time_distributed_5_sparse_categorical_accuracy: 0.3355\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 81s 12s/step - loss: 3.9630 - time_distributed_4_loss: 0.9888 - time_distributed_5_loss: 2.9742 - time_distributed_4_sparse_categorical_accuracy: 0.7728 - time_distributed_5_sparse_categorical_accuracy: 0.3355\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 83s 12s/step - loss: 3.8653 - time_distributed_4_loss: 0.9175 - time_distributed_5_loss: 2.9478 - time_distributed_4_sparse_categorical_accuracy: 0.7728 - time_distributed_5_sparse_categorical_accuracy: 0.3355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4W2Uw-FklyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef594def-5c0a-4594-e80a-12333485583f"
      },
      "source": [
        "word1 = \"öbürkünde\"\n",
        "word2 = \"seninleyken\"\n",
        "word3 = \"benimleyken\"\n",
        "word4 = \"benimdi\"\n",
        "word5= \"kendimizdenmişler\"\n",
        "word6 = \"googlelayken\"\n",
        "word7 = \"googleken\"\n",
        "\n",
        "for ww in [word1, word2, word3, word4, word5, word6, word7]:\n",
        "    ptt = xtokenizer.encode(ww, add_special_tokens=True, max_length=30, pad_to_max_length=True )\n",
        "    ptt = np.array(ptt)\n",
        "    pred = model.predict(ptt)\n",
        "    print(ww)\n",
        "    #print(pred)\n",
        "    #print(pred.shape)\n",
        "    #print(pred[0].shape)\n",
        "    lem = pred[0]\n",
        "    anll = pred[1]\n",
        "    xx = lem.argmax(axis=-1).tolist()\n",
        "    chars = tokenizer.sequences_to_texts(xx)\n",
        "    print(chars)\n",
        "    print(\"\\n\")\n",
        "    yy = anll.argmax(axis=-1).tolist()\n",
        "    als = atokenizer.sequences_to_texts(yy)\n",
        "    print(als)\n",
        "    print(\"\\n\")\n",
        "    print(\"-------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "öbürkünde\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "seninleyken\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "benimleyken\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "benimdi\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "kendimizdenmişler\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "googlelayken\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n",
            "googleken\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs7s12fPRSAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAB1OpNmWWBy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}